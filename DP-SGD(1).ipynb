{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd7e386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "MAX_GRAD_NORM = 1.2\n",
    "EPSILON = 50.0\n",
    "DELTA = 1e-10\n",
    "EPOCHS = 3\n",
    "\n",
    "LR = 1e-3\n",
    "\n",
    "BATCH_SIZE = 120\n",
    "MAX_PHYSICAL_BATCH_SIZE = 256\n",
    "from torchvision.models.mobilenet import mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0ffb9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "DATA_ROOT = '../mnist'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(\n",
    "    root=DATA_ROOT, train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "test_dataset = MNIST(\n",
    "    root=DATA_ROOT, train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(len(train_dataset))\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "images[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e3632a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    top1_acc = []\n",
    "    \n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_loader, \n",
    "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n",
    "        optimizer=optimizer\n",
    "    ) as memory_safe_data_loader:\n",
    "\n",
    "        for i, (images, target) in enumerate(memory_safe_data_loader):   \n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            labels = target.detach().cpu().numpy()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc = accuracy(preds, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            top1_acc.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            if (i+1) % 200 == 0:\n",
    "                epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "                print(\n",
    "                    f\"\\tTrain Epoch: {epoch} \\t\"\n",
    "                    f\"Loss: {np.mean(losses):.6f} \"\n",
    "                    f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
    "                    f\"(ε = {epsilon:.5f}, δ = {DELTA})\"\n",
    "                )\n",
    "    return np.mean(top1_acc), epsilon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d41576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    top1_acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, target in test_loader:\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            labels = target.detach().cpu().numpy()\n",
    "            acc = accuracy(preds, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            top1_acc.append(acc)\n",
    "\n",
    "    top1_avg = np.mean(top1_acc)\n",
    "\n",
    "    print(\n",
    "        f\"\\tTest set:\"\n",
    "        f\"Loss: {np.mean(losses):.6f} \"\n",
    "        f\"Acc: {top1_avg * 100:.6f} \"\n",
    "    )\n",
    "    return np.mean(top1_acc)\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "report = []\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "#for epoch in tqdm(range(EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
    "#    results = train(model, train_loader, optimizer, epoch + 1, device)\n",
    "#    report.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "839fe75c-8c8b-464d-ab0d-ecbe8a1c18d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from opacus.validators import ModuleValidator\n",
    "model = models.resnet18(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f41fa37-57a1-44a9-84b4-5615eda28fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "G_h =['eps_check','distortion','clip','q','k','theta']\n",
    "GU_h =['eps_check','distortion','clip','q', 'a', 'b', 'k','theta', 'M']\n",
    "#G250 = pd.read_csv(\"../arguments/G250_values.csv\", names = G_h)\n",
    "#GU250 = pd.read_csv(\"../arguments/GU_10Evalues.csv\", names=GU_h)\n",
    "#N250 = pd.read_csv(\"../arguments/N250_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49da3a2-f179-4029-b735-e7440d69e051",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>eps_check</th>\n",
       "      <th>distortion</th>\n",
       "      <th>clip</th>\n",
       "      <th>q</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>k</th>\n",
       "      <th>theta</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65546</th>\n",
       "      <td>8810</td>\n",
       "      <td>4.956052</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>37.626263</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>1234.476227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65550</th>\n",
       "      <td>18158</td>\n",
       "      <td>4.957413</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>37.626263</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>1234.816610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65547</th>\n",
       "      <td>27506</td>\n",
       "      <td>4.956372</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>37.626263</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>1234.556202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65548</th>\n",
       "      <td>55550</td>\n",
       "      <td>4.956790</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>37.626263</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>1234.660738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65551</th>\n",
       "      <td>36854</td>\n",
       "      <td>4.957832</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>37.626263</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>1234.921142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65498</th>\n",
       "      <td>346806</td>\n",
       "      <td>4.904135</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.495360</td>\n",
       "      <td>0.014758</td>\n",
       "      <td>1436.799387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65499</th>\n",
       "      <td>356154</td>\n",
       "      <td>4.904553</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.495360</td>\n",
       "      <td>0.014758</td>\n",
       "      <td>1436.903886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65501</th>\n",
       "      <td>365502</td>\n",
       "      <td>4.905100</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.495360</td>\n",
       "      <td>0.014758</td>\n",
       "      <td>1437.040479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65504</th>\n",
       "      <td>384198</td>\n",
       "      <td>4.906748</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.495360</td>\n",
       "      <td>0.014758</td>\n",
       "      <td>1437.452402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65502</th>\n",
       "      <td>374850</td>\n",
       "      <td>4.905814</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.495360</td>\n",
       "      <td>0.014758</td>\n",
       "      <td>1437.219022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  eps_check  distortion  clip       q         a         b  \\\n",
       "65546    8810   4.956052    0.000635   0.1  0.0024  0.000000  0.000010   \n",
       "65550   18158   4.957413    0.000638   0.1  0.0024  0.000010  0.000013   \n",
       "65547   27506   4.956372    0.000638   0.1  0.0024  0.000000  0.000013   \n",
       "65548   55550   4.956790    0.000642   0.1  0.0024  0.000000  0.000017   \n",
       "65551   36854   4.957832    0.000642   0.1  0.0024  0.000010  0.000017   \n",
       "...       ...        ...         ...   ...     ...       ...       ...   \n",
       "65498  346806   4.904135    0.014843   0.1  0.0024  0.000013  0.000085   \n",
       "65499  356154   4.904553    0.014843   0.1  0.0024  0.000017  0.000085   \n",
       "65501  365502   4.905100    0.014843   0.1  0.0024  0.000022  0.000085   \n",
       "65504  384198   4.906748    0.014843   0.1  0.0024  0.000038  0.000085   \n",
       "65502  374850   4.905814    0.014843   0.1  0.0024  0.000029  0.000085   \n",
       "\n",
       "               k     theta            M  \n",
       "65546  37.626263  0.000625  1234.476227  \n",
       "65550  37.626263  0.000625  1234.816610  \n",
       "65547  37.626263  0.000625  1234.556202  \n",
       "65548  37.626263  0.000625  1234.660738  \n",
       "65551  37.626263  0.000625  1234.921142  \n",
       "...          ...       ...          ...  \n",
       "65498   1.495360  0.014758  1436.799387  \n",
       "65499   1.495360  0.014758  1436.903886  \n",
       "65501   1.495360  0.014758  1437.040479  \n",
       "65504   1.495360  0.014758  1437.452402  \n",
       "65502   1.495360  0.014758  1437.219022  \n",
       "\n",
       "[179 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPSILON = 5\n",
    "\n",
    "GU_h =['eps_check','distortion','clip','q', 'a', 'b', 'k','theta', 'M']\n",
    "GU250 = pd.read_csv(\"../arguments/GU_values.csv\", names=GU_h)\n",
    "GU250 = GU250.sort_values(\"eps_check\")\n",
    "GU250.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "GU250 = GU250.dropna()\n",
    "GU250 = GU250.reset_index()\n",
    "GU250.loc[(GU250['eps_check']  <= EPSILON) & (GU250['eps_check']  > EPSILON-0.1)].sort_values(\"distortion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a4e6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.89541495212608"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GU250['eps_check'][69313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e60f63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>eps_check</th>\n",
       "      <th>distortion</th>\n",
       "      <th>clip</th>\n",
       "      <th>q</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>k</th>\n",
       "      <th>theta</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38231</th>\n",
       "      <td>9058</td>\n",
       "      <td>0.493228</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>41.464646</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>118.869099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38261</th>\n",
       "      <td>18406</td>\n",
       "      <td>0.494589</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>41.464646</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>119.209482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38238</th>\n",
       "      <td>27754</td>\n",
       "      <td>0.493548</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>41.464646</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>118.949074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38268</th>\n",
       "      <td>37102</td>\n",
       "      <td>0.495007</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>41.464646</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>119.314014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38247</th>\n",
       "      <td>55798</td>\n",
       "      <td>0.493966</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>41.464646</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>119.053611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38346</th>\n",
       "      <td>375140</td>\n",
       "      <td>0.499103</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.709987</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>120.337926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38314</th>\n",
       "      <td>347096</td>\n",
       "      <td>0.497424</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.709987</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>119.918291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38320</th>\n",
       "      <td>356444</td>\n",
       "      <td>0.497842</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.709987</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>120.022791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38307</th>\n",
       "      <td>337748</td>\n",
       "      <td>0.497105</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.709987</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>119.838345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38168</th>\n",
       "      <td>383889</td>\n",
       "      <td>0.490556</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.307671</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>118.201220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  eps_check  distortion  clip       q         a         b  \\\n",
       "38231    9058   0.493228    0.000065   0.1  0.0024  0.000000  0.000010   \n",
       "38261   18406   0.494589    0.000068   0.1  0.0024  0.000010  0.000013   \n",
       "38238   27754   0.493548    0.000068   0.1  0.0024  0.000000  0.000013   \n",
       "38268   37102   0.495007    0.000072   0.1  0.0024  0.000010  0.000017   \n",
       "38247   55798   0.493966    0.000072   0.1  0.0024  0.000000  0.000017   \n",
       "...       ...        ...         ...   ...     ...       ...       ...   \n",
       "38346  375140   0.499103    0.001381   0.1  0.0024  0.000029  0.000085   \n",
       "38314  347096   0.497424    0.001381   0.1  0.0024  0.000013  0.000085   \n",
       "38320  356444   0.497842    0.001381   0.1  0.0024  0.000017  0.000085   \n",
       "38307  337748   0.497105    0.001381   0.1  0.0024  0.000010  0.000085   \n",
       "38168  383889   0.490556    0.001738   0.1  0.0024  0.000038  0.000085   \n",
       "\n",
       "               k     theta           M  \n",
       "38231  41.464646  0.000055  118.869099  \n",
       "38261  41.464646  0.000055  119.209482  \n",
       "38238  41.464646  0.000055  118.949074  \n",
       "38268  41.464646  0.000055  119.314014  \n",
       "38247  41.464646  0.000055  119.053611  \n",
       "...          ...       ...         ...  \n",
       "38346   1.709987  0.001296  120.337926  \n",
       "38314   1.709987  0.001296  119.918291  \n",
       "38320   1.709987  0.001296  120.022791  \n",
       "38307   1.709987  0.001296  119.838345  \n",
       "38168   1.307671  0.001653  118.201220  \n",
       "\n",
       "[215 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GU250.loc[(GU250['eps_check']  <= 0.5) & (GU250['eps_check']  > 0.49)].sort_values(\"distortion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94812723",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Begin training 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knil/Documents/NextCloud/RAstuff/opacus/opacus/privacy_engine.py:100: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbacacc3927a49798db9fde8fef86144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knil/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/home/knil/Documents/NextCloud/RAstuff/opacus/opacus/accountants/analysis/rdp_plrv.py:106: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return n/d\n",
      "/home/knil/Documents/NextCloud/RAstuff/opacus/opacus/accountants/analysis/rdp_plrv.py:204: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "\tTrain Epoch: 1 \tLoss: 2.497953 Acc@1: 10.235172 (ε = 0.07017, δ = 1e-10)\n",
      "400\n",
      "\tTrain Epoch: 1 \tLoss: 2.405777 Acc@1: 10.640255 (ε = 0.07406, δ = 1e-10)\n",
      "621\n",
      "\tTrain Epoch: 2 \tLoss: 2.148291 Acc@1: 16.253793 (ε = 0.07835, δ = 1e-10)\n",
      "821\n",
      "\tTrain Epoch: 2 \tLoss: 2.101546 Acc@1: 17.786332 (ε = 0.08223, δ = 1e-10)\n",
      "1042\n",
      "\tTrain Epoch: 3 \tLoss: 2.135186 Acc@1: 25.924064 (ε = 0.08652, δ = 1e-10)\n",
      "1242\n",
      "\tTrain Epoch: 3 \tLoss: 2.326306 Acc@1: 28.552486 (ε = 0.09040, δ = 1e-10)\n",
      "1463\n",
      "\tTrain Epoch: 4 \tLoss: 2.539620 Acc@1: 34.999095 (ε = 0.09469, δ = 1e-10)\n",
      "1663\n",
      "\tTrain Epoch: 4 \tLoss: 2.635543 Acc@1: 37.543101 (ε = 0.09857, δ = 1e-10)\n",
      "1884\n",
      "\tTrain Epoch: 5 \tLoss: 2.796383 Acc@1: 44.069685 (ε = 0.10286, δ = 1e-10)\n",
      "2084\n",
      "\tTrain Epoch: 5 \tLoss: 2.771918 Acc@1: 45.152382 (ε = 0.10674, δ = 1e-10)\n"
     ]
    }
   ],
   "source": [
    "acc_plrv_2 = []\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "for i in [1]:\n",
    "    torch.cuda.empty_cache()\n",
    "    args ={\n",
    "        \"a1\":1,\n",
    "        \"a3\":1,\n",
    "        \"a4\":1,\n",
    "        \"lam\":1,\n",
    "        \"moment\":1,\n",
    "        \"theta\":0.00001,\n",
    "        'k':32.6884422110553,\n",
    "        'mu':0,\n",
    "        'sigma':0.5,\n",
    "        'a':GU250['a'][i],\n",
    "        'b':GU250['b'][i],\n",
    "        'u':1,\n",
    "        'l':0.1,\n",
    "        'epsilon':1,\n",
    "        'max_grad_norm': 2,\n",
    "        'gamma':True,\n",
    "        'uniform':False,\n",
    "        'truncnorm':False,\n",
    "    }\n",
    "    print(GU250['theta'][i])\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 119\n",
    "    #print()\n",
    "    from torchvision.datasets import MNIST\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "    DATA_ROOT = '../mnist'\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = CIFAR10(\n",
    "        root=DATA_ROOT, train=True, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    test_dataset = CIFAR10(\n",
    "        root=DATA_ROOT, train=False, download=True, transform=transform)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    #gen_args = find_values(15, i)\n",
    "    runs = []\n",
    "    trun = []\n",
    "\n",
    "    model = mobilenet_v2(num_classes=10)\n",
    "    #model.classifier[1] = torch.nn.Linear(in_features=model.classifier[1].in_features, out_features=10)\n",
    "    #model.conv1 = nn.Conv2d(1, 28, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    #model.load_state_dict(dic)\n",
    "    #model.train()\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ModuleValidator.fix(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    privacy_engine = PrivacyEngine(accountant = 'rdp_plrv')\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "    print(\"Begin training \" + str(i))\n",
    "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            noise_multiplier = 1,\n",
    "            epochs=EPOCHS,\n",
    "            target_epsilon=3,\n",
    "            target_delta=DELTA,\n",
    "            max_grad_norm=GU250['clip'][i],\n",
    "            PLRV_args=args,\n",
    "    )\n",
    "\n",
    "    plrv_report_acc = []\n",
    "    plrv_report_ep = []\n",
    "\n",
    "    for epoch in tqdm(range(EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
    "        acc, ep = train(model, train_loader, optimizer, epoch + 1, device)\n",
    "        plrv_report_acc.append(acc)\n",
    "        plrv_report_ep.append(ep)\n",
    "\n",
    "    acc_plrv_2.append(plrv_report_ep)\n",
    "    del model\n",
    "    del optimizer\n",
    "    #del results\n",
    "#    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3144a75d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000624865811116\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Begin training 65546\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "5.48834758965589e-05\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Begin training 38231\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n"
     ]
    }
   ],
   "source": [
    "acc_plrv_2 = []\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "for i in [65546, 38231]:\n",
    "    torch.cuda.empty_cache()\n",
    "    args ={\n",
    "        \"a1\":1,\n",
    "        \"a3\":1,\n",
    "        \"a4\":1,\n",
    "        \"lam\":1,\n",
    "        \"moment\":1,\n",
    "        \"theta\":GU250['theta'][i],\n",
    "        'k':GU250['k'][i],\n",
    "        'mu':0,\n",
    "        'sigma':0.5,\n",
    "        'a':GU250['a'][i],\n",
    "        'b':GU250['b'][i],\n",
    "        'u':1,\n",
    "        'l':0.1,\n",
    "        'epsilon':1,\n",
    "        'max_grad_norm': GU250['clip'][i],\n",
    "        'gamma':True,\n",
    "        'uniform':True,\n",
    "        'truncnorm':False,\n",
    "    }\n",
    "    print(GU250['theta'][i])\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 119\n",
    "    #print()\n",
    "    from torchvision.datasets import MNIST\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "    DATA_ROOT = '../mnist'\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = CIFAR10(\n",
    "        root=DATA_ROOT, train=True, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    test_dataset = CIFAR10(\n",
    "        root=DATA_ROOT, train=False, download=True, transform=transform)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    #gen_args = find_values(15, i)\n",
    "    runs = []\n",
    "    trun = []\n",
    "\n",
    "    model = mobilenet_v2(num_classes=10)\n",
    "    #model.classifier[1] = torch.nn.Linear(in_features=model.classifier[1].in_features, out_features=10)\n",
    "    #model.conv1 = nn.Conv2d(1, 28, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    #model.load_state_dict(dic)\n",
    "    #model.train()\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ModuleValidator.fix(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    privacy_engine = PrivacyEngine(accountant = 'rdp_plrv')\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "    print(\"Begin training \" + str(i))\n",
    "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            noise_multiplier = 1,\n",
    "            epochs=EPOCHS,\n",
    "            target_epsilon=3,\n",
    "            target_delta=DELTA,\n",
    "            max_grad_norm=GU250['clip'][i],\n",
    "            PLRV_args=args,\n",
    "    )\n",
    "\n",
    "    plrv_report_acc = []\n",
    "    plrv_report_ep = []\n",
    "    privacy_engine.accountant.sample_rate = 1/len(train_loader)\n",
    "    epsis = []\n",
    "    for j in range(len(train_loader)*5):\n",
    "        privacy_engine.accountant.history = [[args, j]]\n",
    "        epsis.append(privacy_engine.get_epsilon(10e-10))\n",
    "    acc_plrv_2.append(epsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de25fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_plrv_2 = [[(10.827088,0.96580), \n",
    "              (27.407932,1.96362),\n",
    "              (41.499464,2.96142),\n",
    "              (47.529392,3.95914),\n",
    "              (52.126053,4.95723),\n",
    "              ], [(10.192589,0.10905),\n",
    "              (16.807797,0.20513),\n",
    "              (27.030008,0.30121),\n",
    "              (38.087848,0.39730),\n",
    "              (46.760616,0.49338),]\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66e8051f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52.126053, 4.95723)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(acc_plrv_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e14e64d9-24a3-4d49-9cc6-5f6caff1ccca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf2cd73eff84d58b191dc412fb841b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "\tTrain Epoch: 1 \tLoss: 2.463116 Acc@1: 9.921548 (ε = 0.06642, δ = 1e-10)\n",
      "400\n",
      "\tTrain Epoch: 1 \tLoss: 2.392245 Acc@1: 10.218374 (ε = 0.06654, δ = 1e-10)\n",
      "621\n",
      "\tTrain Epoch: 2 \tLoss: 2.090827 Acc@1: 21.345325 (ε = 0.06669, δ = 1e-10)\n",
      "821\n",
      "\tTrain Epoch: 2 \tLoss: 2.124887 Acc@1: 26.013381 (ε = 0.06681, δ = 1e-10)\n",
      "1042\n",
      "\tTrain Epoch: 3 \tLoss: 2.299551 Acc@1: 34.715880 (ε = 0.06695, δ = 1e-10)\n",
      "1242\n",
      "\tTrain Epoch: 3 \tLoss: 2.414221 Acc@1: 35.714143 (ε = 0.06708, δ = 1e-10)\n",
      "1463\n",
      "\tTrain Epoch: 4 \tLoss: 2.553982 Acc@1: 39.605806 (ε = 0.06722, δ = 1e-10)\n",
      "1663\n",
      "\tTrain Epoch: 4 \tLoss: 2.609270 Acc@1: 40.977427 (ε = 0.06735, δ = 1e-10)\n",
      "1884\n",
      "\tTrain Epoch: 5 \tLoss: 2.679561 Acc@1: 44.054921 (ε = 0.06749, δ = 1e-10)\n",
      "2084\n",
      "\tTrain Epoch: 5 \tLoss: 2.665736 Acc@1: 44.984952 (ε = 0.06761, δ = 1e-10)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'G250' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m     plrv_report_acc\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[1;32m     62\u001b[0m     plrv_report_ep\u001b[38;5;241m.\u001b[39mappend(ep)\n\u001b[0;32m---> 64\u001b[0m acc_plrv\u001b[38;5;241m.\u001b[39mappend((\u001b[43mG250\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistortion\u001b[39m\u001b[38;5;124m'\u001b[39m][i], test(model, test_loader, device)))\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m optimizer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G250' is not defined"
     ]
    }
   ],
   "source": [
    "acc_plrv = []\n",
    "for i in range(1, len(GU250), int(len(GU250)/10)):\n",
    "    torch.cuda.empty_cache()\n",
    "    args ={\n",
    "        \"a1\":1,\n",
    "        \"a3\":1,\n",
    "        \"a4\":1,\n",
    "        \"lam\":1,\n",
    "        \"moment\":1,\n",
    "        \"theta\":GU250['theta'][i],\n",
    "        'k':GU250['k'][i],\n",
    "        'mu':0,\n",
    "        'sigma':0.5,\n",
    "        'a':GU250['a'][i],\n",
    "        'b':GU250['b'][i],\n",
    "        'u':1,\n",
    "        'l':0.1,\n",
    "        'epsilon':1,\n",
    "        'max_grad_norm': GU250['b'][i],\n",
    "        'gamma':True,\n",
    "        'uniform':False,\n",
    "        'truncnorm':False,\n",
    "    }\n",
    "    \n",
    "    #gen_args = find_values(15, i)\n",
    "    runs = []\n",
    "    trun = []\n",
    "\n",
    "    model = mobilenet_v2(num_classes=10)\n",
    "    #model.classifier[1] = torch.nn.Linear(in_features=model.classifier[1].in_features, out_features=10)\n",
    "    #model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    #model.load_state_dict(dic)\n",
    "    #model.train()\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ModuleValidator.fix(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    privacy_engine = PrivacyEngine(accountant = 'rdp_plrv')\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "    print(\"Begin training \" + str(i))\n",
    "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            noise_multiplier = 1,\n",
    "            epochs=10,\n",
    "            target_epsilon=0.15,\n",
    "            target_delta=DELTA,\n",
    "            max_grad_norm=GU250['clip'][i],\n",
    "            PLRV_args=args,\n",
    "    )\n",
    "\n",
    "    plrv_report_acc = []\n",
    "    plrv_report_ep = []\n",
    "\n",
    "    for epoch in tqdm(range(5), desc=\"Epoch\", unit=\"epoch\"):\n",
    "        acc, ep = train(model, train_loader, optimizer, epoch + 1, device)\n",
    "        plrv_report_acc.append(acc)\n",
    "        plrv_report_ep.append(ep)\n",
    "\n",
    "    acc_plrv.append((G250['distortion'][i], test(model, test_loader, device)))\n",
    "    del model\n",
    "    del optimizer\n",
    "    #del results\n",
    "#    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b566b3-23ad-4f3f-8110-77e3a7dcd7b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_plrv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e14de1-6d68-4f93-a5e5-ca223a9f102f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_eps = pd.read_csv('../arguments/gaussian_eps.csv', names = ['eps'])\n",
    "g_dist = pd.read_csv('../arguments/gaussian_dist.csv', names = ['distortion'])\n",
    "g_clip = pd.read_csv('../arguments/gaussian_clip.csv', names = ['clip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e0357-8abb-4e3a-8057-2a1f8ec567b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_clip = g_clip.dropna()\n",
    "g_clip = g_clip.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17f880-d8b1-4879-8d6b-7eff2cc64cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_rdp = []\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "for i in [0.5, 5]:\n",
    "    print(i)\n",
    "    #gen_args = find_values(15, i)\n",
    "    runs = []\n",
    "    trun = []\n",
    "\n",
    "    model = mobilenet_v2(num_classes=10)\n",
    "    #model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    #model.load_state_dict(dic)\n",
    "    #model.train()\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ModuleValidator.fix(model)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    privacy_engine = PrivacyEngine(accountant = 'rdp')\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "    print(\"Begin training \" + str(i))\n",
    "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            #noise_multiplier = 1,\n",
    "            epochs=5,\n",
    "            target_epsilon=i,\n",
    "            target_delta=1e-10,\n",
    "            max_grad_norm=10,\n",
    "            #PLRV_args=convert_params(gen_args),\n",
    "    )\n",
    "\n",
    "    plrv_report_acc = []\n",
    "    plrv_report_ep = []\n",
    "\n",
    "    for epoch in tqdm(range(5), desc=\"Epoch\", unit=\"epoch\"):\n",
    "        acc, ep = train(model, train_loader, optimizer, epoch + 1, device)\n",
    "        plrv_report_acc.append(acc)\n",
    "        plrv_report_ep.append(ep)\n",
    "\n",
    "    acc_rdp.append((plrv_report_acc, plrv_report_ep))\n",
    "    del model\n",
    "    del optimizer\n",
    "    #del results\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    privacy_engine.accountant.sample_rate = 1/len(train_loader)\n",
    "    epsis = []\n",
    "    for i in range(len(train_loader)*5):\n",
    "        privacy_engine.accountant.history = [[args, i]]\n",
    "        epsis.append(privacy_engine.get_epsilon(10e-10))\n",
    "    acc_rdp.append(epsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe8277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opacus.accountants.utils import get_noise_multiplier\n",
    "acc_rdp = []\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "for i in [0.5, 5]:\n",
    "    print(i)\n",
    "    #gen_args = find_values(15, i)\n",
    "    runs = []\n",
    "    trun = []\n",
    "\n",
    "    model = mobilenet_v2(num_classes=10)\n",
    "    #model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    #model.load_state_dict(dic)\n",
    "    #model.train()\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ModuleValidator.fix(model)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    privacy_engine = PrivacyEngine(accountant = 'rdp')\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "    print(\"Begin training \" + str(i))\n",
    "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            #noise_multiplier = 1,\n",
    "            epochs=5,\n",
    "            target_epsilon=i,\n",
    "            target_delta=1e-10,\n",
    "            max_grad_norm=10,\n",
    "            #PLRV_args=convert_params(gen_args),\n",
    "    )\n",
    "\n",
    "    plrv_report_acc = []\n",
    "    plrv_report_ep = []\n",
    "    \n",
    "    nm = noise_multiplier=get_noise_multiplier(\n",
    "                target_epsilon=i,\n",
    "                target_delta=DELTA,\n",
    "                sample_rate=1/len(train_loader),\n",
    "                epochs=EPOCHS,\n",
    "                accountant=privacy_engine.accountant.mechanism(),\n",
    "            )\n",
    "    print(nm)\n",
    "    #privacy_engine.accountant.sample_rate = BATCH_SIZE/50000\n",
    "    epsis = []\n",
    "    for j in range(len(train_loader)*5):\n",
    "        privacy_engine.accountant.history = [(nm, 1/len(train_loader), j)]\n",
    "        epsis.append(privacy_engine.get_epsilon(DELTA))\n",
    "    acc_rdp.append(epsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae904d1",
   "metadata": {},
   "outputs": [],
   "source": [
    " acc_rdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9876e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rdp = [[(10.116650,0.43046), \n",
    "              (10.711590,0.44728),\n",
    "              (10.492322,0.46410),\n",
    "              (10.288952,0.48092),\n",
    "              (10.566023,0.49774),\n",
    "              ], [(10.417261,4.40004),\n",
    "              (10.726745,4.60714),\n",
    "              (11.139498,4.75362),\n",
    "              (12.236695,4.87810),\n",
    "              (12.436983,4.99017),]\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5a83c-e729-448c-8dfc-833a2c798182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model = models.resnet18(num_classes=10)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model = ModuleValidator.fix(model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "privacy_engine = PrivacyEngine(accountant = 'rdp_plrv')\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            noise_multiplier = 1,\n",
    "            epochs=10,\n",
    "            target_epsilon=0.15,\n",
    "            target_delta=DELTA,\n",
    "            max_grad_norm=GU250['clip'][i],\n",
    "            PLRV_args=args,\n",
    "    )\n",
    "dic = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71412da4-6223-4f15-8c5e-e26f1da511ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_x = [i[0] for i in acc_plrv_2]\n",
    "acc_y = [i[1] for i in acc_plrv_2]\n",
    "dist_g = [i[0] for i in acc_rdp]\n",
    "acc_g = [i[1] for i in acc_rdp]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"distortion\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"distortion vs accuracy\")\n",
    "plt.yscale('log')\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(dist_x, acc_y, 'bo-', label=\"PLRV\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "#ax1.set_yscale('log')\n",
    "ax2 = ax1.twiny()\n",
    "ax2.plot(dist_g, acc_g, 'ro-', label=\"Guassian\")\n",
    "#ax2.set_yscale('log')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470d76b-c3c9-4092-a258-82fcf93419e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_plrv_2.sort()\n",
    "acc_rdp.sort()\n",
    "dist_x = [i[2] for i in acc_plrv_2]\n",
    "acc_y = [i[1] for i in acc_plrv_2]\n",
    "dist_g = [i[0] for i in acc_rdp]\n",
    "acc_g = [i[1] for i in acc_rdp]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"distortion\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"distortion vs accuracy\")\n",
    "plt.yscale('log')\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(dist_x, acc_y, 'bo-', label=\"PLRV\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_xscale('log')\n",
    "ax2 = ax1.twiny()\n",
    "ax2.plot(dist_g, acc_g, 'ro-', label=\"Guassian\")\n",
    "#ax2.set_yscale('log')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c4d84-868d-4345-b497-ceee9b77bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "#acc_plrv_2.sort()\n",
    "#acc_rdp.sort()\n",
    "x_vals = range(1,6)\n",
    "print(acc_plrv_2)\n",
    "plrv1 = [i[1] for i in acc_plrv_2[0]]\n",
    "plrv2 = [i[1] for i in acc_plrv_2[1]]\n",
    "print(acc_rdp)\n",
    "g1 = [i[1] for i in acc_rdp[0]]\n",
    "g2 = [i[1] for i in acc_rdp[1]]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.title(\"Epochs vs Epsilon\")\n",
    "#plt.plot(x_vals, plrv1, 'b+-', label=\"PLRV 45% accuracy\")\n",
    "#plt.plot(x_vals, plrv2, 'bo--', label=\"PLRV 10% accuracy\")\n",
    "plt.plot(x_vals, g1, 'ro-', label=\"gaussian ϵ=0.5\")\n",
    "#plt.plot(x_vals, g2, 'r+-', label=\"gaussian ϵ=5\")\n",
    "plt.plot(x_vals, plrv2, 'bo-', label=\"PLRV  ϵ=0.5\")\n",
    "#plt.plot(x_vals, plrv1, 'b+-', label=\"PLRV  ϵ=5\")\n",
    "#plt.yticks(np.logspace(0, 200, 5)) \n",
    "#plt.yscale(\"log\")\n",
    "#plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x:.2f}'))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa389250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "#acc_plrv_2.sort()\n",
    "#acc_rdp.sort()\n",
    "x_vals = range(1,2106)\n",
    "\n",
    "plrv1 = acc_plrv_2[0]\n",
    "plrv2 = acc_plrv_2[1]\n",
    "g2 = acc_rdp[1]\n",
    "g1 = acc_rdp[0]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.title(\"Steps vs Epsilon\")\n",
    "#plt.plot(x_vals, plrv1, 'b+-', label=\"PLRV 45% accuracy\")\n",
    "#plt.plot(x_vals, plrv2, 'bo--', label=\"PLRV 10% accuracy\")\n",
    "plt.plot(x_vals, g1, 'r--', label=\"gaussian ϵ=0.5\")\n",
    "plt.plot(x_vals, g2, 'r-', label=\"gaussian ϵ=5\")\n",
    "plt.plot(x_vals, plrv2, 'b--', label=\"PLRV  ϵ=0.5\")\n",
    "plt.plot(x_vals, plrv1, 'b-', label=\"PLRV  ϵ=5\")\n",
    "#plt.yticks(np.logspace(0, 200, 5)) \n",
    "#plt.yscale(\"log\")\n",
    "#plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x:.2f}'))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "#acc_plrv_2.sort()\n",
    "#acc_rdp.sort()\n",
    "x_vals = range(1,len(train_loader)+1)\n",
    "\n",
    "plrv1 = acc_plrv_2[0][0:len(train_loader)]\n",
    "plrv2 = acc_plrv_2[1][0:len(train_loader)]\n",
    "g2 = acc_rdp[1][0:len(train_loader)]\n",
    "g1 = acc_rdp[0][0:len(train_loader)]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.title(\"Steps vs Epsilon\")\n",
    "#plt.plot(x_vals, plrv1, 'b+-', label=\"PLRV 45% accuracy\")\n",
    "#plt.plot(x_vals, plrv2, 'bo--', label=\"PLRV 10% accuracy\")\n",
    "plt.plot(x_vals, g1, 'r--', label=\"gaussian ϵ=0.5\")\n",
    "plt.plot(x_vals, g2, 'r-', label=\"gaussian ϵ=5\")\n",
    "plt.plot(x_vals, plrv2, 'b--', label=\"PLRV  ϵ=0.5\")\n",
    "plt.plot(x_vals, plrv1, 'b-', label=\"PLRV  ϵ=5\")\n",
    "#plt.yticks(np.logspace(0, 200, 5)) \n",
    "#plt.yscale(\"log\")\n",
    "#plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x:.2f}'))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7626b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "#acc_plrv_2.sort()\n",
    "#acc_rdp.sort()\n",
    "x_vals = range(1,len(train_loader)+1)\n",
    "\n",
    "plrv1 = acc_plrv_2[0][0:len(train_loader)]\n",
    "plrv2 = acc_plrv_2[1][0:len(train_loader)]\n",
    "g2 = acc_rdp[1][0:len(train_loader)]\n",
    "g1 = acc_rdp[0][0:len(train_loader)]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.title(\"Steps vs Epsilon\")\n",
    "#plt.plot(x_vals, plrv1, 'b+-', label=\"PLRV 45% accuracy\")\n",
    "#plt.plot(x_vals, plrv2, 'bo--', label=\"PLRV 10% accuracy\")\n",
    "plt.plot(x_vals, g1, 'r--', label=\"gaussian ϵ=0.5\")\n",
    "#plt.plot(x_vals, g2, 'r-', label=\"gaussian ϵ=5\")\n",
    "plt.plot(x_vals, plrv2, 'b--', label=\"PLRV  ϵ=0.5\")\n",
    "#plt.plot(x_vals, plrv1, 'b-', label=\"PLRV  ϵ=5\")\n",
    "#plt.yticks(np.logspace(0, 200, 5)) \n",
    "#plt.yscale(\"log\")\n",
    "#plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x:.2f}'))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69866d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23362f-bed5-4c37-9ac4-fc61fd00a3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_plrv = []\n",
    "for i in range(1, 11):\n",
    "    gen_args = find_values(13, i)\n",
    "    runs = []\n",
    "    trun = []\n",
    "\n",
    "    #model = models.resnet18(num_classes=10)\n",
    "    #model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model.load_state_dict(dic)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    privacy_engine = PrivacyEngine(accountant = 'rdp_plrv')\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "    print(\"Begin training \" + str(i))\n",
    "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            noise_multiplier = 1,\n",
    "            epochs=EPOCHS,\n",
    "            target_epsilon=0.15,\n",
    "            target_delta=DELTA,\n",
    "            max_grad_norm=i+1,\n",
    "            PLRV_args=convert_params(gen_args),\n",
    "    )\n",
    "\n",
    "    plrv_report_acc = []\n",
    "    plrv_report_ep = []\n",
    "\n",
    "    for epoch in tqdm(range(1), desc=\"Epoch\", unit=\"epoch\"):\n",
    "        acc, ep = train(model, train_loader, optimizer, epoch + 1, device)\n",
    "        plrv_report_acc.append(acc)\n",
    "        plrv_report_ep.append(ep)\n",
    "\n",
    "    acc_plrv.append(test(model, test_loader, device))\n",
    "#    del model\n",
    "    del optimizer\n",
    "    #del results\n",
    "#    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4c6e1-dadd-4cc0-8aa9-6717a5847a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "3**False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d8669-d93e-4ae5-9e14-58beadd4dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "lm.LinearRegression().fit(X, plrv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ab683",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [50000/120*1,50000/120*2,50000/120*3,50000/120*4,50000/120*5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9111ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate the slope and intercept of the linear regression line\n",
    "def linear_regression(x, y):\n",
    "    # Using the formula for linear regression: y = mx + b\n",
    "    # m = (N * Σ(xy) - Σx * Σy) / (N * Σ(x^2) - (Σx)^2)\n",
    "    # b = (Σy - m * Σx) / N\n",
    "\n",
    "    N = len(x)\n",
    "    Σx = np.sum(x)\n",
    "    Σy = np.sum(y)\n",
    "    Σxy = np.sum(x * y)\n",
    "    Σx2 = np.sum(x ** 2)\n",
    "\n",
    "    # Calculate slope (m) and intercept (b)\n",
    "    m = (N * Σxy - Σx * Σy) / (N * Σx2 - Σx ** 2)\n",
    "    b = (Σy - m * Σx) / N\n",
    "\n",
    "    return m, b\n",
    "\n",
    "# Function to predict y values based on the linear model\n",
    "def predict(x, m, b):\n",
    "    return m * x + b\n",
    "\n",
    "# Example input data (x and y)\n",
    "x = np.array(X)\n",
    "y = np.array(plrv1)\n",
    "\n",
    "# Calculate the slope and intercept\n",
    "m, b = linear_regression(x, y)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Linear regression equation: y = {m:.2f}x + {b:.2f}\")\n",
    "\n",
    "# Predict y values based on the regression line\n",
    "y_pred = predict(x, m, b)\n",
    "\n",
    "# Plot the data points and the regression line\n",
    "plt.scatter(x, y, color='blue')\n",
    "plt.plot(x, y_pred, color='red', label=f'Regression line: y = {m:.2E}x + {b:.2f}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('PLRV Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcccce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(X)\n",
    "y = np.array(plrv2)\n",
    "\n",
    "# Calculate the slope and intercept\n",
    "m, b = linear_regression(x, y)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Linear regression equation: y = {m:.2f}x + {b:.2f}\")\n",
    "\n",
    "# Predict y values based on the regression line\n",
    "y_pred = predict(x, m, b)\n",
    "\n",
    "# Plot the data points and the regression line\n",
    "plt.scatter(x, y, color='blue', label='Data points')\n",
    "plt.plot(x, y_pred, color='red', label=f'Regression line: y = {m:.2E}x + {b:.2f}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760697f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(X)\n",
    "y = np.log(np.array(g1))\n",
    "\n",
    "# Calculate the slope and intercept\n",
    "m, b = linear_regression(x, y)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Linear regression equation: y = {m:.2E}x + {b:.2f}\")\n",
    "\n",
    "# Predict y values based on the regression line\n",
    "y_pred = predict(x, m, b)\n",
    "\n",
    "# Plot the data points and the regression line\n",
    "plt.scatter(x, y, color='blue', label='Data points')\n",
    "plt.plot(x, y_pred, color='red', label=f'Regression line: y = {m:.2f}ln(x) + {b:.2f}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5924fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.log(np.array(X))\n",
    "y = np.array(g2)\n",
    "\n",
    "# Calculate the slope and intercept\n",
    "m, b = linear_regression(x, y)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Linear regression equation: y = {m:.2E}ln(x) + {b:.2f}\")\n",
    "\n",
    "# Predict y values based on the regression line\n",
    "y_pred = predict(x, m, b)\n",
    "\n",
    "# Plot the data points and the regression line\n",
    "plt.scatter(x, y, color='blue', label='Data points')\n",
    "plt.plot(x, y_pred, color='red', label=f'Regression line: y = {m:.2f}ln(x) + {b:.2f}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f673911",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(acc_plrv_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184240e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Question NLI dataset\n",
    "dataset = load_dataset(\"snli\")\n",
    "# Load the Roberta tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding=True)\n",
    "\n",
    "# Apply preprocessing to train, validation, and test sets\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example: example['labels'] != -1)\n",
    "\n",
    "# Load the Roberta model for sequence classification\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
    "\n",
    "# Define the metric for evaluation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = torch.argmax(torch.tensor(predictions), dim=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",# Evaluate after every epoch\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=lambda data: tokenizer.pad(data, padding=True, return_tensors=\"pt\")  # Use dynamic padding\n",
    ")\n",
    "\n",
    "trainer.args.device = \"cpu\"\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "\n",
    "# Print the final test results\n",
    "print(f\"Test results: {test_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa668141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(tokenized_datasets['train']['labels']))  # Check label values in the training set\n",
    "print(set(tokenized_datasets['validation']['labels']))  # Check label values in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tokenized_datasets['train']['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b0c68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "# Ensure that the model runs on the CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the Question NLI dataset\n",
    "dataset = load_dataset(\"snli\")\n",
    "\n",
    "# Load the Roberta tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the premise and hypothesis\n",
    "    return tokenizer(\n",
    "        examples['premise'], \n",
    "        examples['hypothesis'], \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        max_length=512  # Ensure input size consistency\n",
    "    )\n",
    "\n",
    "# Apply preprocessing to train, validation, and test sets\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Ensure that the labels are correctly passed as \"labels\" in the dataset\n",
    "# In SNLI, the labels are already present in the 'label' field. We rename it to 'labels'\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Remove samples with label -1 (if they exist)\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example: example['labels'] != -1)\n",
    "\n",
    "# Check to ensure there are no remaining -1 labels\n",
    "print(set(tokenized_datasets['train']['labels']))  # Check the label values after filtering\n",
    "\n",
    "# Load the Roberta model for sequence classification\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
    "\n",
    "# Move the model to CPU\n",
    "#model.to(device)\n",
    "\n",
    "# Define the metric for evaluation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = torch.argmax(torch.tensor(predictions), dim=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Set up training arguments to run on CPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "    save_strategy=\"epoch\",        # Save after every epoch\n",
    "    per_device_train_batch_size=8,  # Lower batch size for CPU\n",
    "    per_device_eval_batch_size=8,   # Lower batch size for CPU\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    use_cpu=False,  # Disable CUDA to force CPU\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "\n",
    "# Print the final test results\n",
    "print(f\"Test results: {test_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c227e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers[torch] scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda392d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
